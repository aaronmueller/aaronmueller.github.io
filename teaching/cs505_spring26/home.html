<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">

    <meta property="og:site_name" content="CS505 (Natural Language Processing)">
    <meta property="og:type" content="article">
    <meta property="og:title" content="CS505 (Natural Language Processing)">
    <meta property="og:description" content="Getting computers to understand and use language">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="CS505 (Natural Language Processing)">
    <meta name="twitter:description" content="Getting computers to understand and use language">
    <meta name="twitter:url" content="https://aaronmueller.github.io/teaching/cs505_spring26.html">

    <title>CS505: Natural Language Processing</title>

    <link rel="stylesheet" href="../../css/bootstrap.min.css">

    <link href="../../css/fonts.css" rel="stylesheet" type="text/css">

    <link rel="stylesheet" type="text/css" href="../../css/style.css">
    <link rel="stylesheet" href="../../css/font-awesome.min.css">

    <link rel="shortcut icon" href="../../files/bu_logo.ico"/>

    <style>
        .tc-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
            margin: 30px 0;
        }
        
        .outer-list {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }
        
        .outer-list > li {
            margin-bottom: 30px;
        }
        
        .outer-list > li > strong {
            display: block;
            margin-bottom: 8px;
            font-size: 1.1em;
        }
        
        .inner-list {
            list-style-type: disc;
            padding-left: 20px;
            margin: 0;
        }
        
        .inner-list li {
            margin-bottom: 5px;
        }
    </style>

</head>

<body data-new-gr-c-s-check-loaded="14.1063.0" data-gr-ext-installed="">

<nav class="navbar navbar-default navbar-fixed-top">
    <div class="container">
        <div class="navbar-header">
            <a class="navbar-brand brand" href="cs505_spring26.html">CS505</a>
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
                    data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li><a href="#logistics">Logistics</a></li>
                <li><a href="#news">News</a></li>
                <li><a href="#schedule">Schedule</a></li>
                <li><a href="#topics">Topics</a></li>
                <li><a href="#grading">Grading</a></li>
                <!-- <li><a href="#conduct">Policies</a></li>! -->
            </ul>
        </div>
    </div>
</nav>

<div id="header" style="text-align:center">
    <br><br><br>
    <h1>CS505: Natural Language Processing</h1>
        <a href="https://www.bu.edu/cs/">
        <img src="../../files/bu_logo.png" class="logo-right" width="30%">
    </a>
    <h3>Spring 2026</h3>
    <div style="clear:both;"></div>
</div>

<div class="container sec" id="intro">
    <p>
        How can we get computers to understand and generate human language? This is among
        the most challenging&mdash;and currently, the most quickly advancing&mdash;approaches in contemporary artificial
        intelligence. Natural language systems are deployed in the world in increasingly many forms: chatbots,
        code assistants, web agents, among others. This course provides an introduction to the engineering and
        science that underlies current NLP systems.
    </p>

    <p>
        <strong>Prerequisites</strong>
    <ul>
        <li>
            Probability
        </li>
        <li>
            Proficiency in Python programming
        </li>
        <li>
            Linear algebra
        </li>
        <li>
            Calculus
        </li>
    </ul>
    </p>
    <p>
        <strong>Highly recommended prerequisites</strong>:
        Not required, but it will be very useful to have taken a machine learning course before taking this one. Check out these resources to help get you acquainted with the basics:
        <ul>
            <li><a href="https://docs.pytorch.org/tutorials/beginner/pytorch_with_examples.html">The official PyTorch tutorial</a></li>
            <li><a href="https://cs230.stanford.edu/blog/pytorch/">A more brief PyTorch tutorial</a></li>
            <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
            <li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">This short online book about neural networks and deep learning</a> (Chapters 1, 2, and 5), and <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">this video series by 3Blue1Brown on neural networks and backpropagation</a> (Chapters 1 through 4)</li>
            <li><a href="https://huggingface.co/learn/llm-course/en/chapter1/1">The HuggingFace course</a></li>
        </ul>
    </p>
    
    <p>
    <strong>Learning objectives</strong><br>Students will:
    <ol>
        <li>Gain exposure to foundational ideas in NLP.</li>
        <li>Understand the theory underlying current NLP ideas.
        <li>Learn how to implement each element of the contemporary NLP pipeline.</li>
    </ol>
    </p>
</div>
<hr>
<div class="container sec" id="logistics">
    <h2>Logistics</h2>
    <b>Instructor:</b> Aaron Mueller (<a href="mailto:amueller@bu.edu">amueller@bu.edu</a>)<br>
    <b>Teaching Fellow:</b> Ge Gao (<a href="mailto:ggao02@bu.edu">ggao02@bu.edu</a>)<br><br>

    <ul>
        <li><strong>There are no discussion/lab sections! These are cancelled.</strong></li>
        <li><b>Lectures:</b> Tuesdays and Thursdays 3:30pm - 4:45pm, CAS 522</li>
        <li><b>Office hours:</b>
            <ul>
                <li>Tue., 5:00pm - 6:30pm - Aaron (CDS 806)</li>
                <li>Thu., 11:00am - 1:00pm - Ge (Blue collaborative space near CDS 727)</li>
                <li>Thu., 5:00pm - 6:00pm - Ge (On Zoom; see "Helpful links" pinned post on Piazza for link)</li>
                <li>Fri., 2:00pm - 3:30pm - Aaron (CDS 806)</li>
            </ul>
        <li><b>News and announcements:</b> All news and announcements will be made in class and posted on this page.</li>
        <li><b>Changes:</b> The instructor reserves the right to make changes to the syllabus or project due dates. These changes will be announced as early as possible.</li>
    </ul>
    <br>
    <ul>
        <li>For questions, <a href="https://piazza.com/bu/spring2026/cs505">join the Piazza!</a> Entry code: <strong>5uzfqw5vwqd</strong></li>
        <li>For uploading homeworks/projects, use <a href="">Gradescope</a>. Entry code: <strong>WNNEKG</strong></li>
    </ul>
    <br>
</div>
<hr>
<div class="container sec" id="news">
    <h2>News</h2>
    Watch this section for homework and project updates! All news will be posted here and announced in class.
</div>
<hr>
<div class="container sec" id="links">
    <h2>Readings</h2>
        <p>We will be loosely following this book throughout the course: <a href="https://web.stanford.edu/~jurafsky/slp3/">Jurafsky & Manning (J&M)</a>, available online for free.
        You may find it helpful to do these readings to prepare questions for lecture or to review the content in more depth after lecture. Content in the J&M book may appear on the exam; however,
        you will not be expected to know the topics in the textbook that are not covered in lecture.
        </p>
        <p>I will also provide links to optional readings and resources related to the content we cover in class. These can be found in the schedule below. These are to supplement the course
            material for those interested in reading further.
        </p>
</div>
<hr>

<div class="container sec" id="schedule" style="margin-top:0px">
    <br>
    <h2>Course Schedule</h2>
    <p>Note: we will almost definitely alter this schedule! Order may also change depending on the availability of guests.</p>

    <table class="table table-striped">
        <colgroup>
            <col style="width:20%">
            <col style="width:25%">
            <col style="width:25%">
            <col style="width:30%">
        </colgroup>
        <thead>
        <tr class="active">
            <th>Date</th>
            <th>Topic</th>
            <th>Homework</th>
            <th>Readings</th>
        </tr>
        </thead>
        <tbody>
        <tr>
            <td>Jan 20, 2026</td>
            <td>Course introduction
                <ul>
                    <li>What is NLP?</li>
                    <li>Overview of course topics</li>
                </ul>
                <a href="slides/nlp_20jan.pdf">[Slides]</a>
            </td>
            <td><a href="homeworks/hw-1/prereq_review.pdf"> Prereq review released (ungraded)</a><br><br><a href="homeworks/hw-1/prereq_review_solutions.pdf">Prereq review solutions</a></td>
            <td><i>No readings.</i></td>
        </tr>
        <tr>
            <td>Jan 22, 2026</td>
            <td>Text classification
                <ul>
                    <li>Logistic regression</li>
                    <li>Gradient descent</li>
                    <li>Features</li>
                    <li>Machine learning basics</li>
                </ul>
                <a href="slides/nlp_22jan.pdf">[Slides]</a>
            </td>
            <td><strong><a href="homeworks/hw0/cs505_hw0.pdf">HW0 released</strong> <a href="homeworks/hw0/hw0_code.zip">[Code and data]</a></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf">Chapter 4</a></li>
                    <li>Optional:</li>
                    <ul>
                        <li>Review of linear algebra: <a href="https://see.stanford.edu/materials/aimlcs229/cs229-linalg.pdf">Kolter (2017)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Jan 27, 2026</td>
            <td>
                Introduction to language modeling
                <br><br>
                Tokenization
                <ul>
                    <li>The type-token distinction</li>
                    <li>Feature engineering</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/2.pdf">Chapter 2</a></li>
                    <li>Optional:</li>
                    <ul>
                        <li>BPE: <a href="https://aclanthology.org/P16-1162/">Sennrich et al. (2016)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Jan 29, 2026</td>
            <td>Sequence modeling
                <ul>
                    <li>Review of probability theory</li>
                    <li>N-grams</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">Chapter 3</a></li>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/C.pdf">Appendix C</a> (mistakenly called B in the text)</li>
                    <li>Optional:</li>
                    <ul>
                        <li>Review of probability: <a href="https://cs229.stanford.edu/section/cs229-prob.pdf">Maleki & Do</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        
        <tr>
            <td>Feb 3, 2026</td>
            <td>Neural sequence modeling I
                <ul>
                    <li>Feed-forward neural networks</li>
                    <li>Backpropagation</li>
                    <li>Embeddings</li>
                </ul>
            </td>
            <td><strong>HW0 due</strong><br><br><strong>HW1 released</strong></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf">Chapter 5</a></li>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">Chapter 6</a></li>
                    <li>Optional:</li>
                    <ul>
                        <li>Backpropagation: <a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">Rumelhart et al. (1986)</a></li>
                        <li>word2vec: <a href="https://arxiv.org/abs/1301.3781">Mikolov et al. (2013)</a></li>
                        <li>GloVe: <a href="https://aclanthology.org/D14-1162/">Pennington et al. (2014)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Feb 5, 2026</td>
            <td>Neural sequence modeling II
                <ul>
                    <li>Recurrent neural networks</li>
                    <li>LSTMs</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/13.pdf">Chapter 13</a></li>
                    <li>Optional:</li>
                    <ul>
                        <li>Great blog post on RNN language models: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy (2015)</a></li>
                        <li>RNN cheatsheet: <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks">Stanford CS 230</a></li>
                        <li>RNN LMs:<a href="https://www.fit.vut.cz/research/group/speech/public/publi/2010/mikolov_interspeech2010_IS100722.pdf">Mikolov et al. (2010)</a></li>
                        <li>LSTMs: <a href="https://deeplearning.cs.cmu.edu/S23/document/readings/LSTM.pdf">Hochreiter & Schmidhuber (1997)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Feb 10, 2026</td>
            <td>Attention
                <ul>
                    <li>Transformers</li>
                    <li>Parallel processing</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">Chapter 8</a></li>
                    <li>Optional:</li>
                    <ul>
                        <li>The Illustrated Transformer: <a href="https://jalammar.github.io/illustrated-transformer/">Alammar (2018)</a></li>
                        <li>Transformers: <a href="https://arxiv.org/abs/1706.03762">Vaswani et al. (2017)</a></li>
                        <li>The first big attention paper: <a href="https://arxiv.org/abs/1409.0473">Bahdanau et al. (2014)</a></li>
                        <li>More attention: <a href="https://aclanthology.org/D15-1166/">Luong et al. (2015)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Feb 12, 2026</td>
            <td>Large language models I
                <ul>
                    <li>Pre-training</li>
                    <li>Autoregressive language models: GPT</li>
                    <li>Scaling laws</li>
                    <li>Generation methods</li>
                </ul>
            </td>
            <td><strong>HW1 due</strong><br><br><strong>HW2 released</strong></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf">Chapter 7</a></li>
                    <li>Optional:</li>
                    <ul>
                        <li>GPT-2: <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Radford*, Wu* et al. (2019)</a></li>
                        <li>GPT-3: <a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al. (2020)</a></li>
                        <li>Scaling laws: <a href="https://arxiv.org/abs/2001.08361">Kaplan et al. (2020)</a></li>
                        <li>Nucleus sampling: <a href="https://arxiv.org/abs/1904.09751">Holtzman et al. (2019)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Feb 17, 2026</td>
            <td>Large language models II
                <ul>
                    <li>Masked language models: BERT</li>
                    <li>Sequence-to-sequence models: T5</li>
                </ul>
            </td>
            <td></td>
            <td><ul>
                <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf">Chapter 10</a></li>
                <li>Optional:</li>
                <ul>
                    <li>BERT: <a href="https://arxiv.org/abs/1810.04805">Devlin et al. (2018)</a></li>
                    <li>T5: <a href="https://arxiv.org/abs/1910.10683">Raffel et al. (2019)</a></li>
                </ul>
            </ul></td>
        </tr>
        <tr>
            <td>Feb 19, 2026</td>
            <td>Language model use and adaptation
                <ul>
                    <li>Prompting</li>
                    <li>In-context learning</li>
                    <li>Fine-tuning</li>
                    <li>Continued pre-training</li>
                    <li>Low-rank adaptation (LoRA)</li>
                </ul>
            </td>
            <td></td>
            <td><ul>
                <li>Optional:</li>
                <ul>
                    <li>A good continued pre-training method: <a href="https://arxiv.org/abs/2004.10964">Gururangan et al. (2020)</a></li>
                    <li>Why do demonstrations help? <a href="https://aclanthology.org/2022.emnlp-main.759/">Min et al. (2022)</a></li>
                    <li>Chain of thought: <a href="https://arxiv.org/abs/2201.11903">Wei et al. (2022)</a></li>
                    <li>LoRA: <a href="https://arxiv.org/abs/2106.09685">Hu et al. (2021)</a></li>
                </ul>
            </ul></td>
        </tr>
        <tr>
            <td>Feb 24, 2026</td>
            <td>
                Machine translation
                <ul>
                    <li>Linguistic typology</li>
                    <li>Beam search</li>
                    <li>Evaluation: BLEU, human rating, embedding-based methods</li></li>
                </ul>
            </td>
            <td></td>
            <td><ul>
                <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/12.pdf">Chapter 12</a></li>
                <li>Optional:</li>
                <ul>
                    <li>Statistical phrase-based machine translation: <a href="https://aclanthology.org/N03-1017.pdf">Koehn et al. (2003)</a></li>
                    <li>Learning to translate by prompting an LM with a grammar book: <a href="https://arxiv.org/abs/2309.16575">Tanzer et al. (2023)</a></li>
                    <li>BLEU: <a href="https://aclanthology.org/P02-1040.pdf">Papineni et al. (2002)</a></li>
                    <li>The first neural MT approach to outperform statistical approaches: <a href="https://arxiv.org/abs/1409.0473">Bahdanau et al. (2014)</a></li>
                    <li>A recent survey of MT research: <a href="https://www.mdpi.com/2078-2489/16/9/723">Ataman et al. (2025)</a></li>
                </ul>
            </ul></td>
        </tr>
        <tr class="sechighlight5">
            <td>Feb 26, 2026</td>
            <td>Post-training I
                <ul>
                    <li>Instruction tuning</li>
                    <li>Intro to reinforcement learning</li>
                    <li>Reinforcement learning from human feedback (RLHF)</li>
                </ul>
            </td>
            <td>HW2 due</td>
            <td><ul>
                <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Chapter 9</a></li>
                <li>Optional:</li>
                <ul>
                    <li>Instruction tuning: <a href="https://arxiv.org/abs/2109.01652">Wei et al. (2021)</a></li>
                    <li>RLHF: <a href="https://arxiv.org/abs/2203.02155">Ouyang et al. (2022)</a></li>
                    <li>Quick guide to key concepts in reinforcement learning: <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html">OpenAI post</a></li>
                </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Mar 3, 2026</td>
            <td>
                Post-training II
                <ul>
                    <li>Direct preference optimization (DPO)</li>
                    <li>Reinforcement learning with verifiable rewards (RLVR)</li>
                </ul>
            </td>
            <td><strong>Final project proposal released</strong></td>
            <td>
                <ul>
                    <li>Optional, <i>but highly recommended</i>:</li>
                    <ul>
                        <li>DPO: <a href="https://arxiv.org/abs/2305.18290">Rafailov et al. (2024)</a></li>
                        <li>RLVR (GRPO): <a href="https://arxiv.org/abs/2402.03300">Shao et al. (2024)</a></li>
                        <li>DeepSeek: <a href="https://arxiv.org/abs/2501.12948">Guo et al. (2025)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Mar 5, 2026</td>
            <td>Morphology and syntax I
                <ul>
                    <li>Intro to morphology and syntax</li>
                    <li>Sequence tagging</li>
                    <li>POS tagging</li>
                    <li>Hidden Markov models (HMMs)</li>
                </ul>
            </td>
            <td><strong>HW3 released</strong></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/17.pdf">Chapter 17</a></li>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/A.pdf">Appendix A</a></li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Mar 10, 2026</td>
            <td>
                <p style="color: red;"><i>Spring break - no class</i></p>
            </td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>Mar 12, 2026</td>
            <td>
                <p style="color: red;"><i>Spring break - no class</i></p>
                <td></td>
                <td></td>
            </td>
        </tr>
        <tr>
            <td>Mar 17, 2026</td>
            <td>
                Morphology and syntax II
                <ul>
                    <li>(Probabilistic) context-free grammars</li>
                    <li>Constituency parsing</li>
                    <li>CKY</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/18.pdf">Chapter 18</a></li>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/E.pdf">Appendix E</a> (through E.2)</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Mar 19, 2026</td>
            <td>
                Morphology and syntax III
                    <ul>
                        <li>Dependency grammars</li>
                        <li>Dependency parsing</li>
                        <li>Shift-reduce</li>
                    </ul>
                Overview of exam topics
            </td>
            <td><strong>HW3 due <i>Friday, Mar. 20</i></strong></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/19.pdf">Chapter 19</a></li>
                </ul>
            </td>
        </tr>
        <tr class="sechighlight5">
            <td>Mar 24, 2026</td>
            <td>
                <strong>Exam</strong>
            </td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>Mar 26, 2026</td>
            <td>
                Semantics
                <ul>
                    <li>Argument structure</li>
                    <li>Semantic role labeling</li>
                </ul>
            </td>
            <td></td>
            <td><ul>
                <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/21.pdf">Chapter 21</a></li>
            </ul></td>
        </tr>
        <tr>
            <td>Mar 31, 2026</td>
            <td>
                <ul>
                    <li>Review of exam</li>
                    <li>Discourse and pragmatics</li>
                    <ul>
                        <li>The structure of conversation</li>
                        <li>Coreference resolution</li>
                    </ul>
                </ul>
            </td>
            <td><strong>Final project proposal due</strong></td>
            <td><ul>
                <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/23.pdf">Chapter 23</a></li>
            </ul></td>
        </tr>
        <tr>
            <td>Apr 2, 2026</td>
            <td>
                NLP applications and evaluation
                <ul>
                    <li>The NLP community</li>
                    <li>Paraphrase detection</li>
                    <li>Natural language inference</li>
                    <li>Multiple-choice question answering</li>
                    <li>Open-ended question answering</li>
                </ul>
            </td>
            <td></td>
            <td><ul>
                <li>Optional:</li>
                <ul>
                    <li>GLUE: <a href="https://aclanthology.org/W18-5446/">Wang et al. (2018)</a></li>
                    <li>MMLU: <a href="https://arxiv.org/abs/2009.03300">Hendrycks et al. (2020)</a></li>
                    <li>SQuAD: <a href="https://aclanthology.org/D16-1264/">Rajpurkar et al. (2016)</a></li>
                    <li>Dynabench: <a href="https://aclanthology.org/2021.naacl-main.324/">Kiela et al. (2021)</a></li>
                </ul>
            </ul></td>
        </tr>
        <tr>
            <td>Apr 7, 2026</td>
            <td>
                Interpretability and evaluation I
                <ul>
                    <li>Best practices in evaluation</li>
                    <li>Out-of-distribution generalization</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>Optional:</li>
                    <ul>
                        <li>A survey of generalization research in NLP: <a href="https://www.nature.com/articles/s42256-023-00729-y">Hupkes et al. (2023)</a></li>
                        <li>Syntactic generalization in Transformers: <a href="https://arxiv.org/abs/2404.16367">Ahuja et al. (2024)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Apr 9, 2026</td>
            <td>
                Interpretability and evaluation II
                <ul>
                    <li>(Mechanistic) interpretability</li>
                    <li>Model editing</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>Optional:</li>
                    <ul>
                        <li>Targeted model editing: <a href="https://arxiv.org/abs/2202.05262">Meng et al. (2022)</a></li>
                        <li>Distributed alignment search: <a href="https://arxiv.org/abs/2303.02536">Geiger et al. (2023)</a></li>
                        <li>A guide to saliency map methods: <a href="https://christophm.github.io/interpretable-ml-book/pixel-attribution.html">Molnar (2019)</a></li>
                        <li>A survey of probing papers: <a href="https://aclanthology.org/2022.cl-1.7/">Belinkov (2022)</a></li>
                        <li>A survey of mechanistic interpretability: <a href="https://direct.mit.edu/coli/article/doi/10.1162/COLI.a.572/133277/The-Quest-for-the-Right-Mediator-Surveying">Mueller et al. (2025)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Apr 14, 2026</td>
            <td>
                Retrieval and tool use
            </td>
            <td></td>
            <td>
                <ul>
                    <li>Optional:</li>
                    <ul>
                        <li>Retrieval-augmented generation (RAG): <a href="https://arxiv.org/abs/2005.11401">Lewis et al. (2020)</a></li>
                        <li>Toolformer: <a href="https://arxiv.org/abs/2302.04761">Schick et al. (2023)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr class="sechighlight5">
            <td>Apr 16, 2026</td>
            <td>LLM bias, safety, and fairness</td>
            <td><strong>Midway report due</strong></td>
            <td>
                <ul>
                    <li>Optional:</li>
                    <ul>
                        <li>Implicit biases in LLMs: <a href="https://www.pnas.org/doi/10.1073/pnas.2416228122">Bai et al. (2025)</a></li>
                        <li>A survey on bias and fairness in LLMs: <a href="https://direct.mit.edu/coli/article/50/3/1097/121961/Bias-and-Fairness-in-Large-Language-Models-A">Gallegos et al. (2024)</a></li>
                        <li>Debiasing may not always work as intended: <a href="https://aclanthology.org/N19-1061/">Gonen & Goldberg (2019)</a></li>
                        <li>Controlling hallucinations and toxicity in LLMs: <a href="https://arxiv.org/pdf/2507.21509">Chen et al. (2025)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Apr 21, 2026</td>
            <td>Multimodal NLP
                <ul>
                    <li>Language grounding</li>
                    <li>Vision language models</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>Optional:</li>
                    <ul>
                        <li>Winoground: <a href="https://arxiv.org/abs/2204.03162">Thrush et al. (2022)</a></li>
                        <li>LLaVA: <a href="https://arxiv.org/abs/2304.08485">Liu et al. (2023)</a></li>
                        <li>CLIP: <a href="https://arxiv.org/abs/2103.00020">Radford et al. (2021)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr> 
            <td>Apr 23, 2026</td>
            <td>
                Human language learning and processing
                <ul>
                    <li>Incremental processing and parsing</li>
                    <li>Human language acquisition</li>
                    <li>Cognitively inspired NLP</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>Optional:</li>
                    <ul>
                        <li>Surprisal theory: <a href="https://www.mit.edu/~rplevy/papers/levy-2008-cognition.pdf">Levy (2008)</a></li>
                        <li>How many words do children hear? <a href="https://pubmed.ncbi.nlm.nih.gov/28418456/">Gilkerson et al. (2017)</a></li>
                        <li>BabyLM: <a href="https://aclanthology.org/2023.conll-babylm.1/">Warstadt*, Mueller* et al. (2023)</a></li>
                    </ul>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Apr 28, 2026</td>
            <td>Guest lecture</td>
            <td></td>
            <td></td>
        </tr>
        <tr class="sechighlight4 centered">
            <td>Apr 30, 2026</td>
            <td>Final project help session</td>
            <td></td>
            <td></td>
        </tr>
        <tr class="sechighlight5">
            <td>TBD</td>
            <td></td>
            <td></td>
            <td><strong>Final report due</strong></td>
        </tr>
        </tbody>
    </table>
</div>
<hr>
<br><br>

<div class="container sec", id="topics">
<h2>List of Topics</h2>
<p>By the end of this course, you should be familiar with each of the following topics. Items with an asterisk* may be on the exam.</p>
<div class="tc-container">
    <ul class="outer-list">
        <li>
            The history of natural language processing:
            <ul class="inner-list">
                <li>Claude Shannon and the invention of language models</li>
                <li>The Turing test</li>
                <li>ELIZA</li>
                <li>Statistical methods</li>
                <li>Neural methods</li>
            </ul>
        </li>

        <li>
            Tokenization:
            <ul class="inner-list">
                <li>The type&ndash;token distinction*</li>
                <li>Words*</li>
                <li>Characters*</li>
                <li>Bytes*</li>
                <li>Unicode*</li>
                <li>UTF-8*</li>
                <li>Byte-pair encoding (BPE)*</li>
            </ul>
        </li>

        <li>
            Classification:
            <ul class="inner-list">
                <li>Binary Logistic regression*</li>
                <li>Multinomial logistic regression*</li>
            </ul>
        </li>

        <li>
            Nonlinear activation functions:
            <ul class="inner-list">
                <li>Sigmoid*</li>
                <li>Softmax*</li>
                <li>ReLU*</li>
            </ul>
        </li>

        <li>
            Machine learning basics:
            <ul class="inner-list">
                <li>Loss functions*, including:</li>
                <ul>
                    <li>Cross-entropy*</li>
                    <li>Negative log likelihood*</li>
                </ul>
                <li>Gradients*</li>
                <li>Gradient descent*</li>
                <li>Stochastic gradient descent*</li>
                <li>Learning rates*</li>
                <li>Mini-batching*</li>
                <li>Train-test splits*</li>
            </ul>
        </li>

        <li>
            Neural network architectures:
            <ul class="inner-list">
                <li>Feedforward layers*</li>
                <li>Recurrent neural networks*</li>
                <li>Long short-term memory networks (LSTMs)*</li>
                <li>Transformers*</li>
            </ul>
        </li>

        <li>
            Language model architectures:
            <ul class="inner-list">
                <li>N-gram language models*</li>
                <li>Recurrent neural networks language models*</li>
                <li>Transformer language models*</li>
                <li>Autoregressive language models*</li>
                <li>Masked language models*</li>
                <li>Sequence-to-sequence language models*</li>
            </ul>
        </li>

        <li>
            Language modeling methods:
            <ul class="inner-list">
                <li>Next-token prediction*</li>
                <li>Pre-training*</li>
            </ul>
        </li>
        
        <li>
            NLP tasks:
            <ul class="inner-list">
                <li>Sentiment classification*</li>
                <li>Language identification*</li>
                <li>Spam detection*</li>
                <li>Toxicity detection</li>
                <li>Machine translation*</li>
                <li>Question answering (QA):</li>
                    <ul>
                        <li>Multiple-choice QA</li>
                        <li>Extractive QA</li>
                        <li>Abstractive QA</li>
                    </ul>
            </ul>
        </li>

        <li>
            Generation methods:
            <ul class="inner-list">
                <li>Greedy decoding*</li>
                <li>Top-k sampling*</li>
                <li>Nucleus sampling*</li>
                <li>Beam search*</li>
            </ul>
        </li>

        <li>
            Language model training:
            <ul class="inner-list">
                <li>Pre-training*</li>
                <li>Scaling laws*</li>
            </ul>
        </li>
    </ul>
    
    <ul class="outer-list">
        <li>
            Knowing thy data:
            <ul class="inner-list">
                <li>Corpora and their construction*</li>
                <li>Documents vs. paragraphs vs. sentences*</li>
                <li>Domain shift</li>
                <li>Temporal shift</li>
            </ul>
        </li>
        
        <li>
            Prompting:
            <ul class="inner-list">
                <li>In-context learning*</li>
                <li>Chain-of-thought prompting*</li>
            </ul>
        </li>
        
        <li>
            Fine-tuning and adaptation:
            <ul class="inner-list">
                <li>Continued pre-training*</li>
                <li>Fine-tuning*</li>
                <li>Low-rank adapters*</li>
            </ul>
        </li>

        <li>
            Evaluation:
            <ul class="inner-list">
                <li>Precision, recall, and F1*</li>
                <li>Accuracy*</li>
                <li>Surprisal and perplexity*</li>
                <li>Out-of-distribution (OOD) generalization</li>
            </ul>
        </li>

        <li>
            Reinforcement learning basics:
            <ul class="inner-list">
                <li>Policy*</li>
                <li>Reward functions*</li>
            </ul>
        </li>

        <li>
            Post-training methods:
            <ul class="inner-list">
                <li>Reinforcement learning from human feedback (RLHF)*</li>
                <li>Direct preference optimization (DPO)*</li>
                <li>Reinforcement learning with verifiable rewards (RLVR)*</li>
            </ul>
        </li>
        
        <li>
            Syntax and parsing:
            <ul class="inner-list">
                <li>Constituencies*</li>
                <li>Dependencies*</li>
                <li>Context-free grammars*</li>
                <li>CKY*</li>
                <li>Shift-reduce*</li>
                <li>The context-sensitivity of natural language</li>
            </ul>
        </li>

        <li>
            Morphology:
            <ul class="inner-list">
                <li>Morphemes*</li>
                <li>Analytic vs. synthetic languages</li>
                <li>Fusional vs. agglutinative languages</li>
            </ul>
        </li>

        <li>
            Semantics:
            <ul class="inner-list">
                <li>Semantic role labeling</li>
                <li>Theta roles</li>
            </ul>
        </li>

        <li>
            Discourse:
            <ul class="inner-list">
                <li>Coreference resolution</li>
                <li>Pragmatic inference</li>
            </ul>
        </li>

        <li>
            Multilingual NLP:
            <ul class="inner-list">
                <li>High- vs. low-resource languages</li>
                <li>Linguistic typology</li>
                <li>Language imbalance*</li>
            </ul>
        </li>

        <li>
            Interpretability:
            <ul class="inner-list">
                <li>Saliency maps</li>
                <li>Circuits</li>
                <li>Shortcuts and spurious correlations</li>
                <li>Causation</li>
            </ul>
        </li>

        <li>
            Retrieval:
            <ul class="inner-list">
                <li>Retrieval-augmented generation (RAG)</li>
                <li>Dense passage retrievers (DPR)</li>
            </ul>
        </li>

        <li>
            Agents
            <ul class="inner-list">
                <li>Language models as agents</li>
                <li>Tool use</li>
                <li>Multi-agent frameworks</li>
            </ul>
        </li>

        <li>
            Social impacts and considerations:
            <ul class="inner-list">
                <li>AI safety</li>
                <li>Bias, fairness, and toxicity</li>
                <li>Factuality and hallucinations</li>
            </ul>
        </li>

    </ul>
</div>
</div>

<div class="container sec" id="grading">
    <h2>Grading</h2>
    <p>
        The course is graded out of 100 total points.
    </p>
    

    <h4>Homeworks: 15 points</h4>
    <ul>
        <li>
            <b>Homework 0</b>: 4 points
        </li>
        <li>
            <b>Homework 1</b>: 4 points
        </li>
        <li>
            <b>Homework 2</b>: 4 points
        </li>
        <li>
            <b>Homework 3</b>: 3 points
        </li>
    </ul>
    <p>The homeworks are largely for your benefit as study tools. You may use AI in any way you wish to complete the homeworks, but you will find studying for the exam
        much easier if you understand the methods you'll be implementing in the homeworks. Regardless of whether you decide to use AI tools, you (the student) are fully
        responsible for what you submit.</p>
    <br>

    <h4>Exam: 35 points</h4>
    <p>
        There will be one exam about 2/3 of the way through the course. See the list of topics above for a guide to
        what the exam will cover. You may not use any electronic resources for the exam; this includes the textbook, AI tools, the internet,
        text messages, among other items.
        <br>
        This will be an open-note exam! If you bring notes, they must be on one physical piece of paper. 
        Electronic notes will not be allowed.
    </p>
    <br>

    <h4>Final project: 50 points</h4>
    <p>This is an open-ended project where you will review and pursue an NLP topic of your choosing.</p>
    <ul>
        <li>
            <b>Project proposal: 5 points</b>
            <p>
                Perform a literature review on your topic of choice. Also briefly describe your planned project,
                including the task you'll be focusing on, your data, methods, baselines, and evaluation.
            </p>
        </li>
        <li>
            <b>Midway report: 15 points</b>
            <p>Report your progress on the final project thus far. By this point, you should have run some experiments and obtained some preliminary
                results. Outline your plan for the rest of the project.
            </p>
        </li>
        <li>
            <b>Final report: 30 points</b>
            <p>Describe your experiments, present your results, and report your findings in the style of a typical NLP paper.</p>
        </li>
    </ul>

    <br>
    
    Grading of the final project will be based on the following:
    <ul>
    <li><strong>Proposal</strong>: Well-motivated idea with a concrete experimental plan. Creativity is encouraged more than bulletproof-but-incremental ideas!
    <li><strong>Midway report:</strong>
        <ul>
            <li>Clear problem definition and motivation</li>
            <li>The most essential related work is present</li>
            <li>Reasonable set of initial experiments</li>
            <li>Clear presentation of methods and evaluation protocol</li>
            <li>Clear presentation and description of preliminary results</li>
            <li>Well-reasoned discussion about your initial experiments. If successful, what do the experiments tell us so far? If they didn't turn out how you expected, why do you think this was?</li>
            <li>Concrete plan for the rest of the project. It's ok to pivot from your original plan if needed!</li>
        </ul>
    </li>
    <li><strong>Final report</strong>: Much like the midway report, but in addition:
        <ul>
            <li>The related work should be reasonably complete and well-integrated throughout the report</li>
            <li>More complete methods section</li>
            <li>More complete results section</li>
            <li>Rigorous evaluation</li>
            <li>Discussion and conclusion composed of well-formulated arguments, grounded in your experimental findings and the broader literature</li>
            <li>Novelty and creativity</li>
        </ul>
    </li>
    </ul>

    <p>
        <b><i>Can we publish our final project?</i></b> It is feasible to convert a course project into an academic publication, but it can take a lot of work! I encourage those interested to discuss this with me at the end of the semester.
    </p>
</div>
<hr>

<div class="container sec" id="conduct">
    <h2>Policies and Conduct</h2>

    <br>
    <h4>Outside Resources & AI Policy</h4>
    <p>
        <b>AI tools are completely allowed for the homeworks</b>. I recommend doing the assignments on your own as exam preparation, but for the purpose of grading,
        you can complete the assignments completely with AI if you so choose. It is the student's responsibility to verify any submitted content.
    </p>
    </p>
        <b>AI tools are allowed for the final project</b>. Our policy here is more nuanced: you may use AI as a tool, but do not use AI as a crutch or replacement for
        thinking. What's the difference? AI as a tool includes:
        <ul>
            <li>Using AI to help you workshop your initial ideas</li>
            <li>Using AI to refine your writing, including spell-checking, reorganization, among other copyediting-like actions</li>
            <li>Using AI to help you debug code</li>
        </ul>

        AI as a crutch/replacement includes:
        <ul>
            <li>Using AI to come up with your final project idea for you from scratch</li>
            <li>Using AI to write all of your code</li>
            <li>Using AI to generate your reports</li>
        </ul>
        
        The line between tool and crutch can be fuzzy, so if you're unsure, I recommend asking ahead of time! I promise not to judge
        if you ask before you turn in the assignment. :)
    </p>
    <p>
        <b>No AI tools are allowed during exams.</b> These will be hand-written in class.
    </p>

    <p>I strongly encourage you to use any outside source at your disposal when doing the homework and your final project. Your reports and code should be original, but you may take inspiration from existing papers as long as you give them proper credit. When doing your project, feel free to base your implementations on publicly available code as well (as long as you make significant modifications to accommodate your original idea), but be sure to give proper credit in your report and your GitHub README if you do so.</p>
    <p>
        For the final project, failing to properly cite an outside source is equivalent to taking credit for ideas that are not your own, which is plagiarism. 
    </p>

    <br>
    <h4>Academic Integrity</h4>
    <p>Read through <a href="https://www.bu.edu/provost/students/undergraduate/academic-integrity/bus-academic-conduct-code/">BU's Academic Conduct Code</a>. All students are expected to abide by these guidelines. In the context of this class, it's particularly important that you cite the source of your ideas, facts, and/or methods, and do not claim someone else's work as your own.</p>
    <p>You are allowed to work in groups to do the homeworks, but you should upload your homework individually.</p>
    
    <br>
    <h4>Absence and Late Work Policy</h4>
    <p>Attendance will not be taken. Attend lectures as you wish.</p>
    <p>
        You have <strong>6 free late days</strong> that you can use however you wish with no excuse necessary. Using a late day means that
        you can still receive full credit for the assignment with no late penalty. Turning in an assignment late after using all your late days
        will incur a 10% drop in the score for each late day. 5 days after an assignment's due date, the assignment can no longer be turned in, regardless of whether
        you use your free late days.
        This applies to all homeworks. It also applies to the proposal and midway report for the final project&mdash;<strong>but not the final report</strong>, which must
        be turned in on time. Note that a late day is a step function: turning in a homework 5 minutes late is equivalent to turning it in 23 hours late,
        so if you know you'll be late, we recommend taking the extra time to verify your understanding of the material.
    </p>
    <p>
        Extensions can be negotiated in cases of medical emergency or other sudden pressing circumstances. Students should contact the course staff ASAP and negotiate
        <emph>before the assignment's original due date</emph>. If this applies to the first homework, please come talk to us the first day of class.
    </p>
    <p>
        Exams cannot easily be made up. If you know you cannot make an exam day, <emph>you must notify us at least 14 days in advance</emph> so that we can make
        alternate arrangements.
    </p>

    <br>
    <h4>Accommodations</h4>
    <p>Boston University's policy is to provide reasonable accommodations to students with qualifying disabilities who are enrolled in Boston University courses. Students seeking accommodations must engage in an interactive process with, and provide appropriate documentation of their disability to, Disability & Access Services (DAS). If this applies, please get in touch with me as soon as possible to discuss accommodations; note that students are not required to disclose information regarding their disability, if applicable, but should request approval for such accommodations through DAS beforehand.</p>

    <br>
    <h4>Religious Observance</h4>
    <p>Students are permitted to be absent from class, including classes involving examinations, labs, excursions, and other special events, for purposes of religious observance.  In-class, take-home and lab assignments, and other work shall be made up in consultation with the student's instructors. More details on BU's religious observance policy are available <a href="https://www.bu.edu/chapel/students/religious-holidays-policies/">here</a>.</p>

    <br>
    <h4>Acknowledgments</h4>
    <p>Much of the content in this course was inspired by NLP courses taught by Greg Durrett, John Hewitt, and Jason Eisner. Please do check out their syllabi (available online)
        if you're interested in getting a new perspective on many of these topics!
    </p>
</div>
<br><br>

<script src="files/jquery.min.js"></script>
<script src="files/bootstrap.min.js"></script>

</body>
</html>