<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">

    <meta property="og:site_name" content="CS505 (Natural Language Processing)">
    <meta property="og:type" content="article">
    <meta property="og:title" content="CS505 (Natural Language Processing)">
    <meta property="og:description" content="Getting computers to understand and use language">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="CS505 (Natural Language Processing)">
    <meta name="twitter:description" content="Getting computers to understand and use language">
    <meta name="twitter:url" content="https://aaronmueller.github.io/teaching/cs505_spring26.html">

    <title>CS505: Natural Language Processing</title>

    <link rel="stylesheet" href="../../css/bootstrap.min.css">

    <link href="../../css/fonts.css" rel="stylesheet" type="text/css">

    <link rel="stylesheet" type="text/css" href="../../css/style.css">
    <link rel="stylesheet" href="../../css/font-awesome.min.css">

    <link rel="shortcut icon" href="../../files/bu_logo.ico"/>

    <style>
        .tc-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
        }
        
        .outer-list {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }
        
        .outer-list > li {
            margin-bottom: 30px;
        }
        
        .outer-list > li > strong {
            display: block;
            margin-bottom: 8px;
            font-size: 1.1em;
        }
        
        .inner-list {
            list-style-type: disc;
            padding-left: 20px;
            margin: 0;
        }
        
        .inner-list li {
            margin-bottom: 5px;
        }
    </style>

</head>

<body data-new-gr-c-s-check-loaded="14.1063.0" data-gr-ext-installed="">

<nav class="navbar navbar-default navbar-fixed-top">
    <div class="container">
        <div class="navbar-header">
            <a class="navbar-brand brand" href="cs505_spring26.html">CS505</a>
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
                    data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li><a href="#news">News</a></li>
                <li><a href="#links">Links</a></li>
                <li><a href="#schedule">Schedule</a></li>
                <li><a href="#grading">Grading</a></li>
                <li><a href="#conduct">Policies</a></li>
            </ul>
        </div>
    </div>
</nav>

<div id="header" style="text-align:center">
    <br><br><br>
    <h1>CS505: Natural Language Processing</h1>
        <a href="https://www.bu.edu/cs/">
        <img src="../../files/bu_logo.png" class="logo-right" width="30%">
    </a>
    <h3>Boston University - Spring 2026</h3>
    <div style="clear:both;"></div>
</div>

<div class="container sec" id="intro">
    <p>
        How can we get computers to understand and generate human language? This is among
        the most challenging&mdash;and currently, the most quickly advancing&mdash;approaches in contemporary artificial
        intelligence. Natural language systems are deployed in the world in increasingly many forms: chatbots,
        code assistants, web agents, among others. This course provides an introduction to the engineering and
        science that underlies current NLP systems.
    </p>

    <p>
        <strong>Prerequisites</strong>
    <ul>
        <li>
            Probability
        </li>
        <li>
            Proficiency in Python programming
        </li>
        <li>
            Linear algebra
        </li>
        <li>
            Calculus
        </li>
    </ul>
    </p>
    <p>
        <strong>Highly recommended prerequisites</strong>:
        Not required, but it will be very useful to have taken a machine learning course before taking this one.
        <ul>
            <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
            <li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">This short online book about neural networks and deep learning</a> (Chapters 1, 2, and 5), and <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">this video series by 3Blue1Brown on neural networks and backpropagation</a> (Chapters 1 through 4)</li>
        </ul>
    </p>
    
    <p>
    <strong>Learning objectives</strong><br>Students will:
    <ol>
        <li>Gain exposure to foundational ideas in NLP.</li>
        <li>Understand the theory underlying current NLP ideas.
        <li>Learn how to implement each element of the contemporary NLP pipeline.</li>
    </ol>
    </p>
</div>
<hr>
<div class="container sec" id="logistics">
    <h2>Logistics</h2>
    <ul>
        <li><b>Classes:</b> Tuesdays and Thursdays 3:30pm - 4:45pm, CDS 801</li>
        <li><b>Instructor:</b> Aaron Mueller (amueller@bu.edu)</li>
        <li><b>Teaching Fellows:</b></li>
        <li><b>Office hours:</b> Tuesdays 3pm - 4pm and Wednesdays 12pm - 1pm @ CDS 806, or by appointment</li>
        <li><b>News and announcements:</b> All news and announcements will be made in class and posted on this page.</li>
        <li><b>Changes:</b> The instructor reserves the right to make changes to the syllabus or project due dates. These changes will be announced as early as possible.</li>
    </ul>
    <br>
</div>
<hr>
<div class="container sec" id="news">
    <h2>News</h2>
    Watch this section for homework and project updates!
</div>
<hr>
<div class="container sec" id="links">
    <h2>Book</h2>
        We will be loosely following this book throughout the course: <a href="https://web.stanford.edu/~jurafsky/slp3/">Jurafsky & Manning</a>, available online for free.
</div>
<hr>

<div class="container sec" id="schedule" style="margin-top:0px">
    <br>
    <h2>Course Schedule</h2>
    <p>Note: we will almost definitely alter this schedule! Order may also change depending on the availability of guests.</p>

    <table class="table">
        <colgroup>
            <col style="width:12%">
            <col style="width:20%">
            <col style="width:10%">
            <col style="width:20%">
            <col style="width:15%">
        </colgroup>
        <thead>
        <tr class="active">
            <th>Date</th>
            <th>Topic</th>
            <th>Homework</th>
            <th>Readings</th>
            <th>Notes</th>
        </tr>
        </thead>
        <tbody>
        <tr>
            <td>Jan 20, 2025</td>
            <td>Course introduction
                <ul>
                    <li>What is NLP?</li>
                    <li>Overview of course topics</li>
                </ul>
            </td>
            <td>HW-1 released (ungraded)</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>Jan 22, 2025</td>
            <td>Text classification
                <ul>
                    <li>Logistic regression</li>
                    <li>Gradient descent</li>
                    <li>Features</li>
                    <li>Machine learning basics</li>
                </ul>
            </td>
            <td>HW0 released</td>
            <td>
                <ul>
                    <li>J&M (2025): <a href="https://web.stanford.edu/~jurafsky/slp3/2.pdf">Chapter 2</a></li>
                </ul>
            </td>
            <td></td>
        </tr>
        <tr>
            <td>Jan 27, 2025</td>
            <td>
                Introduction to language modeling
                <br><br>
                Tokenization
                <ul>
                    <li>The type-token distinction</li>
                    <li>Feature engineering</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>J&M (2025): <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">Chapter 3</a></li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Jan 29, 2025</td>
            <td>Sequence modeling
                <ul>
                    <li>Review of probability theory</li>
                    <li>N-grams</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>J&M (2025): <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf">Chapter 4</a></li>
                </ul>
            </td>
        </tr>
        
        <tr>
            <td>Feb 3, 2025</td>
            <td>Neural sequence modeling I
                <ul>
                    <li>Feed-forward neural networks</li>
                    <li>Backpropagation</li>
                    <li>Embeddings</li>
                </ul>
            </td>
            <td>HW0 due<br>HW1 released</td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf">Chapter 5</a></li>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">Chapter 6</a></li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Feb 5, 2025</td>
            <td>Neural sequence modeling II
                <ul>
                    <li>Recurrent neural networks</li>
                    <li>LSTMs</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/13.pdf">Chapter 13</a></li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Feb 10, 2025</td>
            <td>Attention
                <ul>
                    <li>Transformers</li>
                    <li>Parallel processing</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">Chapter 8</a></li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Feb 12, 2025</td>
            <td>Large language models I
                <ul>
                    <li>Pre-training</li>
                    <li>Autoregressive language models: GPT</li>
                </ul>
            </td>
            <td>HW1 due<br>HW2 released</td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf">Chapter 7</a></li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Feb 17, 2025</td>
            <td>Large language models II
                <ul>
                    <li>Masked language models: BERT</li>
                    <li>Sequence-to-sequence models: T5</li>
                </ul>
            </td>
            <td></td>
        </tr>
        <tr>
            <td>Feb 19, 2025</td>
            <td>Evaluating language models</td>
            <td></td>
        </tr>
        <tr>
            <td>Feb 24, 2025</td>
            <td>
                NLP tasks I
                <ul>
                    <li>Prompting</li>
                    <li>In-context learning</li>
                </ul>
            </td>
            <td></td>
        </tr>
        <tr class="sechighlight5">
            <td>Feb 26, 2025</td>
            <td>NLP tasks II
                <ul>
                    <li>Fine-tuning</li>
                    <li>Low-rank adapters</li>
                </ul>
            </td>
            </td>
        </tr>
        <tr>
            <td>Mar 3, 2025</td>
            <td>Post-training I
                <ul>
                    <li>Intro to reinforcement learning</li>
                    <li>Reinforcement learning from human feedback (RLHF)</li>
                </ul>
            </td>
            <td>HW2 due<br>Final project proposal released</td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Chapter 9</a></li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Mar 5, 2025</td>
            <td>
                Post-training II
                <ul>
                    <li>Direct preference optimization (DPO)</li>
                    <li>Reinforcement learning with verifiable rewards (RLVR)</li>
                </ul>
            </td>
            <td></td>
            <td>
                <ul>
                    <li>J&M: <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Chapter 9</a></li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Mar 10, 2025</td>
            <td>
                <p style="color: red;"><i>Spring break - no class</i></p>
            </td>
        </tr>
        <tr>
            <td>Mar 12, 2025</td>
            <td>
                <p style="color: red;"><i>Spring break - no class</i></p>
            </td>
        </tr>
        <tr>
            <td>Mar 17, 2025</td>
            <td>
                Morphology and syntax I
                <ul>
                    <li>Intro to morphology and syntax</li>
                    <li>POS tagging</li>
                    <li>Hidden Markov models (HMMs)</li>
                </ul>
            </td>
            <td>HW3 released</td>
        </tr>
        <tr>
            <td>Mar 19, 2025</td>
            <td>Morphology and syntax II
                <ul>
                    <li>(Probabilistic) context-free grammars</li>
                    <li>Constituency parsing</li>
                    <li>CKY</li>
                </ul>
            </td>
            <td></td>
        </tr>
        <tr class="sechighlight5">
            <td>Mar 24, 2025</td>
            <td>Morphology and syntax III
                <ul>
                    <li>Dependency parsing</li>
                    <li>Shift-reduce</li>
                </ul>
            </td>
            <td></td>
        </tr>
        <tr>
            <td>Mar 26, 2025</td>
            <td>
                <li>Semantics</li>
                <ul>
                    <li>Models of meaning</li>
                    <li>Semantic role labeling</li>
                    <li>Semantic parsing</li>
                </ul>
            <td>HW3 due</td>
        </tr>
        <tr>
            <td>Mar 31, 2025</td>
            <td><strong>Exam</strong></td>
            </td>
        </tr>
        <tr>
            <td>Apr 2, 2025</td>
            <td><ul>
                <li>Review of exam</li>
                <li>Discourse and pragmatics</li>
                <ul>
                    <li>The structure of conversation</li>
                    <li>Coreference resolution</li>
                </ul>
            </td>
            <td>Final project proposal due</td>
        </tr>
        <tr> 
            <td>Apr 7, 2025</td>
            <td>NLP tasks I: Classification
                <ul>
                    <li>Paraphrase detection</li>
                    <li>Natural language inference</li>
                    <li>Multiple-choice question answering</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Apr 9, 2025</td>
            <td>
                NLP tasks II: Generation
                <ul>
                    <li>Machine translation</li>
                    <li>Open-ended question answering</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Apr 14, 2025</td>
            <td>
                Multilingual NLP
                <ul>
                    <li>Multilingual pre-training</li>
                    <li>Low-resource languages</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Apr 16, 2025</td>
            <td>
                Interpretability and evaluation I
                <ul>
                    <li>Best practices in evaluation</li>
                    <li>Out-of-distribution generalization</li>
                </ul>
            </td>
        </tr>
        <tr class="sechighlight5">
            <td>Apr 21, 2025</td>
            <td>
                Interpretability and evaluation II
                <ul>
                    <li>(Mechanistic) interpretability</li>
                    <li>Model editing</li>
                </ul>
            </td>
            <td>Midway report due</td>
        </tr>
        <tr>
            <td>Apr 23, 2025</td>
            <td>
                Retrieval and tool use
            </td>
            <td></td>
        </tr>
        <tr>
            <td>Apr 28, 2025</td>
            <td>Guest lecture</td>
        </tr>
        <tr class="sechighlight4 centered">
            <td>Apr 30, 2025</td>
            <td>Final project help session</td>
        </tr>
        <tr class="sechighlight5">
            <td>TBD</td>
            <td colspan="4"><strong>Final report due</strong></td>
        </tr>
        </tbody>
    </table>
</div>
<hr>
<br><br>

<h2>List of Topics</h2>
<p>By the end of this course, you should be familiar with each of the following topics. Items with an asterisk* may be on the exam.</p>
<div class="tc-container", id="topics">
    <ul class="outer-list">
        <li>
            The history of natural language processing:
            <ul class="inner-list">
                <li>Claude Shannon and the invention of language models</li>
                <li>The Turing test</li>
                <li>ELIZA</li>
                <li>Statistical methods</li>
                <li>Neural methods</li>
            </ul>
        </li>

        <li>
            Tokenization:
            <ul class="inner-list">
                <li>The type&ndash;token distinction*</li>
                <li>Words*</li>
                <li>Characters*</li>
                <li>Bytes*</li>
                <li>Unicode*</li>
                <li>UTF-8*</li>
                <li>Byte-pair encoding (BPE)*</li>
            </ul>
        </li>

        <li>
            Classification:
            <ul class="inner-list">
                <li>Binary Logistic regression*</li>
                <li>Multinomial logistic regression*</li>
            </ul>
        </li>

        <li>
            Nonlinear activation functions:
            <ul class="inner-list">
                <li>Sigmoid*</li>
                <li>Softmax*</li>
            </ul>
        </li>

        <li>
            Machine learning basics:
            <ul class="inner-list">
                <li>Loss functions*, including:</li>
                <ul>
                    <li>Cross-entropy*</li>
                    <li>Negative log likelihood*</li>
                </ul>
                <li>Gradients*</li>
                <li>Gradient descent*</li>
                <li>Stochastic gradient descent*</li>
                <li>Learning rates*</li>
                <li>Mini-batching*</li>
                <li>Train-test splits*</li>
            </ul>
        </li>

        <li>
            Neural network architectures:
            <ul class="inner-list">
                <li>Feedforward layers*</li>
                <li>Recurrent neural networks*</li>
                <li>Long short-term memory networks (LSTMs)*</li>
                <li>Transformers*</li>
            </ul>
        </li>

        <li>
            Language model architectures:
            <ul class="inner-list">
                <li>N-gram language models*</li>
                <li>Recurrent neural networks language models*</li>
                <li>Transformer language models*</li>
                <li>Autoregressive language models*</li>
                <li>Masked language models*</li>
                <li>Sequence-to-sequence language models*</li>
            </ul>
        </li>

        <li>
            Language modeling methods:
            <ul class="inner-list">
                <li>Next-token prediction*</li>
                <li>Pre-training*</li>
            </ul>
        </li>

        <li>
            Language model architectures:
            <ul class="inner-list">
                <li>N-gram language models*</li>
                <li>Recurrent neural networks language models*</li>
                <li>Transformer langiuage models*</li>
            </ul>
        </li>
        
        <li>
            NLP tasks:
            <ul class="inner-list">
                <li>Sentiment classification</li>
                <li>Language identification</li>
                <li>Spam detection</li>
                <li>Toxicity detection</li>
                <li>Machine translation</li>
                <li>Question answering (QA):</li>
                    <ul>
                        <li>Multiple-choice QA</li>
                        <li>Extractive QA</li>
                        <li>Abstractive QA</li>
                    </ul>
                <li>Machine translation</li>
            </ul>
        </li>
    </ul>
    
    <ul class="outer-list">
        <li>
            Knowing thy data:
            <ul class="inner-list">
                <li>Corpora and their construction*</li>
                <li>Documents vs. paragraphs vs. sentences*</li>
                <li>Domain shift*</li>
                <li>Temporal shift*</li>
            </ul>
        </li>
        
        <li>
            Prompting:
            <ul class="inner-list">
                <li>In-context learning*</li>
                <li>Chain-of-thought prompting*</li>
            </ul>
        </li>
        
        <li>
            Fine-tuning and adaptation:
            <ul class="inner-list">
                <li>Continued pre-training*</li>
                <li>Fine-tuning*</li>
                <li>Low-rank adapters*</li>
            </ul>
        </li>

        <li>
            Evaluation:
            <ul class="inner-list">
                <li>Precision, recall, and F1*</li>
                <li>Accuracy*</li>
                <li>Surprisal and perplexity*</li>
                <li>Out-of-distribution (OOD) generalization</li>
            </ul>
        </li>

        <li>
            Reinforcement learning basics:
            <ul class="inner-list">
                <li>Policy*</li>
                <li>Reward functions*</li>
            </ul>
        </li>

        <li>
            Post-training methods:
            <ul class="inner-list">
                <li>Reinforcement learning from human feedback (RLHF)*</li>
                <li>Direct preference optimization (DPO)*</li>
                <li>Reinforcement learning with verifiable rewards (RLVR)*</li>
            </ul>
        </li>
        
        <li>
            Syntax and parsing:
            <ul class="inner-list">
                <li>Constituencies*</li>
                <li>Dependencies*</li>
                <li>Context-free grammars*</li>
                <li>CKY*</li>
                <li>Shift-reduce*</li>
                <li>The context-sensitivity of natural language</li>
            </ul>
        </li>

        <li>
            Morphology:
            <ul class="inner-list">
                <li>Morphemes*</li>
                <li>Analytic vs. synthetic languages</li>
                <li>Fusional vs. agglutinative languages</li>
            </ul>
        </li>

        <li>
            Semantics:
            <ul class="inner-list">
                <li>Semantic role labeling</li>
                <li>Theta roles</li>
            </ul>
        </li>

        <li>
            Discourse:
            <ul class="inner-list">
                <li>Coreference resolution</li>
                <li>Pragmatic inference</li>
            </ul>
        </li>

        <li>
            Multilingual NLP:
            <ul class="inner-list">
                <li>High- vs. low-resource languages</li>
                <li>Linguistic typology</li>
                <li>Language imbalance</li>
            </ul>
        </li>

        <li>
            Interpretability:
            <ul class="inner-list">
                <li>Saliency maps</li>
                <li>Circuits</li>
                <li>Shortcuts and spurious correlations</li>
                <li>Causation</li>
            </ul>
        </li>

        <li>
            Retrieval:
            <ul class="inner-list">
                <li>Retrieval-augmented generation (RAG)</li>
                <li>Dense passage retrievers (DPR)</li>
            </ul>
        </li>

        <li>
            Agents
            <ul class="inner-list">
                <li>Language models as agents</li>
                <li>Tool use</li>
                <li>Multi-agent frameworks</li>
            </ul>
        </li>

        <li>
            Agents
            <ul class="inner-list">
                <li>Tool use</li>
                <li>Multi-agent systems</li>
            </ul>
        </li>

        <li>
            Social impacts and considerations:
            <ul class="inner-list">
                <li>AI safety</li>
                <li>Bias, fairness, and toxicity</li>
                <li>Factuality and hallucinations</li>
            </ul>
        </li>

    </ul>

</div>

<div class="container sec" id="grading">
    <h2>Grading</h2>
    <p>
        The course is graded out of 100 total points.
    </p>
    

    <h4>Homeworks: 15 points</h4>
    <ul>
        <li>
            <b>Homework 0</b>: 4 points
        </li>
        <li>
            <b>Homework 1</b>: 4 points
        </li>
        <li>
            <b>Homework 2</b>: 4 points
        </li>
        <li>
            <b>Homework 3</b>: 3 points
        </li>
    </ul>
    <p>The homeworks are largely for your benefit as study tools. You may use AI in any way you wish to complete the homeworks. Regardless of whether you decide to use AI tools, you are fully responsible for what you submit.</p>
    <br>

    <h4>Exam: 35 points</h4>
    <p>
        There will be one exam about 2/3 of the way through the course. See the list of topics above for a guide to
        what the exam will cover. You may not use any electronic resources for the exam; this includes the textbook, AI tools, the internet,
        text messages, among other items.
        <br>
        This will be an open-note exam! If you bring notes, they must be on one physical piece of paper. 
        Electronic notes will not be allowed.
    </p>
    <br>

    <h4>Final project: 50 points</h4>
    <p>This is an open-ended project where you will review and pursue an NLP topic of your choosing.</p>
    <ul>
        <li>
            <b>Project proposal: 5 points</b>
            <p>
                Perform a literature review on your topic of choice. Also briefly describe your planned project,
                including the task you'll be focusing on, your data, methods, baselines, and evaluation.
            </p>
        </li>
        <li>
            <b>Midway report: 10 points</b>
            <p>Report your progress on the final project thus far. By this point, you should have run some experiments and obtained some preliminary
                results. Outline your plan for the rest of the project.
            </p>
        </li>
        <li>
            <b>Final report: 25 points</b>
            <p>Describe your experiments, present your results, and report your findings in the style of a typical NLP paper.</p>
        </li>
    </ul>

    <br>
    
    Grading of the final project will be based on the following:
    <ul>
    <li><strong>Proposal</strong>: Well-motivated idea with a concrete experimental plan. Creativity is encouraged more than bulletproof-but-incremental ideas!
    <li><strong>Midway report:</strong>
        <ul>
            <li>Clear problem definition and motivation</li>
            <li>The most essential related work is present</li>
            <li>Reasonable set of initial experiments</li>
            <li>Clear presentation of methods and evaluation protocol</li>
            <li>Clear presentation and description of preliminary results</li>
            <li>Well-reasoned discussion about your initial experiments. If successful, what do the experiments tell us so far? If they didn't turn out how you expected, why do you think this was?</li>
            <li>Concrete plan for the rest of the project. It's ok to pivot from your original plan if needed!</li>
        </ul>
    </li>
    <li><strong>Final report</strong>: Much like the midway report, but in addition:
        <ul>
            <li>The related work should be reasonably complete and well-integrated throughout the report</li>
            <li>More complete methods section</li>
            <li>More complete results section</li>
            <li>Rigorous evaluation</li>
            <li>Discussion and conclusion composed of well-formulated arguments, grounded in your experimental findings and the broader literature</li>
            <li>Novelty and creativity</li>
        </ul>
    </li>
    </ul>

    <p>
        <b><i>Can we publish our final project?</i></b> It is feasible to convert a course project into an academic publication, but it can take a lot of work! I encourage those interested to discuss this with me at the end of the semester.
    </p>
</div>
<hr>

<div class="container sec" id="conduct">
    <h2>Policies and Conduct</h2>

    <br>
    <h4>Outside Resources & AI Policy</h4>
    <p>
        <b>AI tools are completely allowed for the homeworks</b>. I recommend doing the assignments on your own as exam preparation, but for the purpose of grading,
        you can complete the assignments completely with AI if you so choose. It is the student's responsibility to verify any submitted content.
    </p>
    </p>
        <b>AI tools are allowed for the final project</b>. Our policy here is more nuanced: you may use AI as a tool, but do not use AI as a crutch or replacement for
        thinking. What's the difference? AI as a tool includes:
        <ul>
            <li>Using AI to help you workshop your initial ideas</li>
            <li>Using AI to refine your writing, including spell-checking, reorganization, among other copyediting-like actions</li>
            <li>Using AI to help you debug code</li>
        </ul>

        AI as a crutch/replacement includes:
        <ul>
            <li>Using AI to come up with your final project idea for you from scratch</li>
            <li>Using AI to write all of your code</li>
            <li>Using AI to generate your reports</li>
        </ul>
        
        The line between tool and crutch can be fuzzy, so if you're unsure, I recommend asking ahead of time! I promise not to judge
        if you ask before you turn in the assignment. :)
    </p>
    <p>
        <b>No AI tools are allowed during exams.</b> These will be hand-written in class.
    </p>

    <p>I strongly encourage you to use any outside source at your disposal when doing the homework and your final project. Your reports and code should be original, but you may take inspiration from existing papers as long as you give them proper credit. When doing your project, feel free to base your implementations on publicly available code as well (as long as you make significant modifications to accommodate your original idea), but be sure to give proper credit in your report and your GitHub README if you do so.</p>
    <p>
        For the final project, failing to properly cite an outside source is equivalent to taking credit for ideas that are not your own, which is plagiarism. 
    </p>

    <br>
    <h4>Academic Integrity</h4>
    <p>Read through <a href="https://www.bu.edu/provost/students/undergraduate/academic-integrity/bus-academic-conduct-code/">BU's Academic Conduct Code</a>. All students are expected to abide by these guidelines. In the context of this class, it's particularly important that you cite the source of your ideas, facts, and/or methods, and do not claim someone else's work as your own.</p>
    
    <br>
    <h4>Absence and Late Work Policy</h4>
    <p>Attendance will not be taken. Attend lectures as you wish.</p>
    <p>
        You have <strong>6 free late days</strong> that you can use however you wish with no excuse necessary. Using a late day means that
        you can still receive full credit for the assignment with no late penalty. Turning in an assignment late after using all your late days
        will incur a 10% drop in the score for each late day. 5 days after an assignment's due date, the assignment can no longer be turned in, regardless of whether
        you use your free late days.
        This applies to all homeworks. It also applies to the proposal and midway report for the final project&mdash;<strong>but not the final report</strong>, which must
        be turned in on time. Note that a late day is a step function: turning in a homework 5 minutes late is equivalent to turning it in 23 hours late,
        so if you know you'll be late, we recommend taking the extra time to verify your understanding of the material.
    </p>
    <p>
        Extensions can be negotiated in cases of medical emergency or other sudden pressing circumstances. Students should contact the course staff ASAP and negotiate
        <emph>before the assignment's original due date</emph>. If this applies to the first homework, please come talk to us the first day of class.
    </p>
    <p>
        Exams cannot easily be made up. If you know you cannot make an exam day, <emph>you must notify us at least 14 days in advance</emph> so that we can make
        alternate arrangements.
    </p>

    <br>
    <h4>Accommodations</h4>
    <p>Boston University's policy is to provide reasonable accommodations to students with qualifying disabilities who are enrolled in Boston University courses. Students seeking accommodations must engage in an interactive process with, and provide appropriate documentation of their disability to, Disability & Access Services (DAS). If this applies, please get in touch with me as soon as possible to discuss accommodations; note that students are not required to disclose information regarding their disability, if applicable, but should request approval for such accommodations through DAS beforehand.</p>

    <br>
    <h4>Religious Observance</h4>
    <p>Students are permitted to be absent from class, including classes involving examinations, labs, excursions, and other special events, for purposes of religious observance.  In-class, take-home and lab assignments, and other work shall be made up in consultation with the student's instructors. More details on BU's religious observance policy are available <a href="https://www.bu.edu/chapel/students/religious-holidays-policies/">here</a>.</p>
</div>
<br><br>

<script src="files/jquery.min.js"></script>
<script src="files/bootstrap.min.js"></script>

</body>
</html>